Base DT Model:
(A) Model Description:
Base DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[102  35 132]
 [ 49 170  58]
 [109  46 135]]
Precision Scores: [0.39230769 0.67729084 0.41538462]
Recall Scores: [0.37918216 0.61371841 0.46551724]
F1 Scores: [0.38563327 0.64393939 0.43902439]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.39      0.38      0.39       269
           F       0.68      0.61      0.64       277
           I       0.42      0.47      0.44       290

    accuracy                           0.49       836
   macro avg       0.49      0.49      0.49       836
weighted avg       0.49      0.49      0.49       836

(D) Accuracy: 0.4868421052631579
(D) Macro-average F1: 0.4895323515015524
(D) Weighted-average F1: 0.4897411902014233
--------------------
Top DT Model:
(A) Model Description:
Top DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[129  29 111]
 [ 46 196  35]
 [126  45 119]]
Precision Scores: [0.42857143 0.72592593 0.4490566 ]
Recall Scores: [0.4795539  0.70758123 0.41034483]
F1 Scores: [0.45263158 0.7166362  0.42882883]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.43      0.48      0.45       269
           F       0.73      0.71      0.72       277
           I       0.45      0.41      0.43       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.53       836
weighted avg       0.53      0.53      0.53       836

(D) Accuracy: 0.5311004784688995
(D) Macro-average F1: 0.5326988684055941
(D) Weighted-average F1: 0.5318498585983786
--------------------
Base MLP Model:
(A) Model Description:
Base MLP Model with hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 66  30 173]
 [ 11 211  55]
 [ 71  50 169]]
Precision Scores: [0.44594595 0.72508591 0.4256927 ]
Recall Scores: [0.24535316 0.76173285 0.58275862]
F1 Scores: [0.31654676 0.74295775 0.49199418]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.45      0.25      0.32       269
           F       0.73      0.76      0.74       277
           I       0.43      0.58      0.49       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.52       836
weighted avg       0.53      0.53      0.52       836

(D) Accuracy: 0.5334928229665071
(D) Macro-average F1: 0.5171662288841662
(D) Weighted-average F1: 0.5186946009696302
--------------------
Top MLP Model:
(A) Model Description:
Top MLP Model with hyperparameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (30, 50), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 24  24 221]
 [  5 203  69]
 [ 24  44 222]]
Precision Scores: [0.45283019 0.74907749 0.43359375]
Recall Scores: [0.08921933 0.73285199 0.76551724]
F1 Scores: [0.14906832 0.74087591 0.55361596]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.45      0.09      0.15       269
           F       0.75      0.73      0.74       277
           I       0.43      0.77      0.55       290

    accuracy                           0.54       836
   macro avg       0.55      0.53      0.48       836
weighted avg       0.54      0.54      0.49       836

(D) Accuracy: 0.5370813397129187
(D) Macro-average F1: 0.4811867318299587
(D) Weighted-average F1: 0.4854911902489731
--------------------
Base DT Model:
(A) Model Description:
Base DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[106  38 125]
 [ 49 167  61]
 [113  45 132]]
Precision Scores: [0.39552239 0.668      0.41509434]
Recall Scores: [0.39405204 0.60288809 0.45517241]
F1 Scores: [0.39478585 0.63377609 0.43421053]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.40      0.39      0.39       269
           F       0.67      0.60      0.63       277
           I       0.42      0.46      0.43       290

    accuracy                           0.48       836
   macro avg       0.49      0.48      0.49       836
weighted avg       0.49      0.48      0.49       836

(D) Accuracy: 0.48444976076555024
(D) Macro-average F1: 0.48759082156573247
(D) Weighted-average F1: 0.48764883108233303
--------------------
Top DT Model:
(A) Model Description:
Top DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[127  29 113]
 [ 46 196  35]
 [125  45 120]]
Precision Scores: [0.4261745  0.72592593 0.44776119]
Recall Scores: [0.47211896 0.70758123 0.4137931 ]
F1 Scores: [0.44797178 0.7166362  0.43010753]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.43      0.47      0.45       269
           F       0.73      0.71      0.72       277
           I       0.45      0.41      0.43       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.53       836
weighted avg       0.53      0.53      0.53       836

(D) Accuracy: 0.5299043062200957
(D) Macro-average F1: 0.5315718352091401
(D) Weighted-average F1: 0.5307940414567187
--------------------
Base MLP Model:
(A) Model Description:
Base MLP Model with hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 66  30 173]
 [ 11 211  55]
 [ 71  50 169]]
Precision Scores: [0.44594595 0.72508591 0.4256927 ]
Recall Scores: [0.24535316 0.76173285 0.58275862]
F1 Scores: [0.31654676 0.74295775 0.49199418]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.45      0.25      0.32       269
           F       0.73      0.76      0.74       277
           I       0.43      0.58      0.49       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.52       836
weighted avg       0.53      0.53      0.52       836

(D) Accuracy: 0.5334928229665071
(D) Macro-average F1: 0.5171662288841662
(D) Weighted-average F1: 0.5186946009696302
--------------------
Top MLP Model:
(A) Model Description:
Top MLP Model with hyperparameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (30, 50), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 84  49 136]
 [ 19 227  31]
 [ 99  62 129]]
Precision Scores: [0.41584158 0.67159763 0.43581081]
Recall Scores: [0.31226766 0.81949458 0.44482759]
F1 Scores: [0.3566879  0.73821138 0.44027304]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.42      0.31      0.36       269
           F       0.67      0.82      0.74       277
           I       0.44      0.44      0.44       290

    accuracy                           0.53       836
   macro avg       0.51      0.53      0.51       836
weighted avg       0.51      0.53      0.51       836

(D) Accuracy: 0.5263157894736842
(D) Macro-average F1: 0.5117241059152183
(D) Weighted-average F1: 0.5120966247833585
--------------------
Base DT Model:
(A) Model Description:
Base DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[104  37 128]
 [ 43 172  62]
 [109  41 140]]
Precision Scores: [0.40625    0.688      0.42424242]
Recall Scores: [0.3866171  0.62093863 0.48275862]
F1 Scores: [0.39619048 0.65275142 0.4516129 ]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.41      0.39      0.40       269
           F       0.69      0.62      0.65       277
           I       0.42      0.48      0.45       290

    accuracy                           0.50       836
   macro avg       0.51      0.50      0.50       836
weighted avg       0.51      0.50      0.50       836

(D) Accuracy: 0.49760765550239233
(D) Macro-average F1: 0.5001849341887293
(D) Weighted-average F1: 0.5004247897646479
--------------------
Top DT Model:
(A) Model Description:
Top DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[129  29 111]
 [ 46 196  35]
 [126  45 119]]
Precision Scores: [0.42857143 0.72592593 0.4490566 ]
Recall Scores: [0.4795539  0.70758123 0.41034483]
F1 Scores: [0.45263158 0.7166362  0.42882883]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.43      0.48      0.45       269
           F       0.73      0.71      0.72       277
           I       0.45      0.41      0.43       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.53       836
weighted avg       0.53      0.53      0.53       836

(D) Accuracy: 0.5311004784688995
(D) Macro-average F1: 0.5326988684055941
(D) Weighted-average F1: 0.5318498585983786
--------------------
Base MLP Model:
(A) Model Description:
Base MLP Model with hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 66  30 173]
 [ 11 211  55]
 [ 71  50 169]]
Precision Scores: [0.44594595 0.72508591 0.4256927 ]
Recall Scores: [0.24535316 0.76173285 0.58275862]
F1 Scores: [0.31654676 0.74295775 0.49199418]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.45      0.25      0.32       269
           F       0.73      0.76      0.74       277
           I       0.43      0.58      0.49       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.52       836
weighted avg       0.53      0.53      0.52       836

(D) Accuracy: 0.5334928229665071
(D) Macro-average F1: 0.5171662288841662
(D) Weighted-average F1: 0.5186946009696302
--------------------
Top MLP Model:
(A) Model Description:
Top MLP Model with hyperparameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (30, 50), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[114  33 122]
 [ 25 221  31]
 [126  55 109]]
Precision Scores: [0.43018868 0.71521036 0.41603053]
Recall Scores: [0.42379182 0.79783394 0.37586207]
F1 Scores: [0.42696629 0.75426621 0.39492754]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.43      0.42      0.43       269
           F       0.72      0.80      0.75       277
           I       0.42      0.38      0.39       290

    accuracy                           0.53       836
   macro avg       0.52      0.53      0.53       836
weighted avg       0.52      0.53      0.52       836

(D) Accuracy: 0.5311004784688995
(D) Macro-average F1: 0.5253866799902703
(D) Weighted-average F1: 0.5242998309878595
--------------------
Base DT Model:
(A) Model Description:
Base DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[107  34 128]
 [ 48 172  57]
 [111  43 136]]
Precision Scores: [0.40225564 0.69076305 0.42367601]
Recall Scores: [0.39776952 0.62093863 0.46896552]
F1 Scores: [0.4        0.6539924  0.44517185]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.40      0.40      0.40       269
           F       0.69      0.62      0.65       277
           I       0.42      0.47      0.45       290

    accuracy                           0.50       836
   macro avg       0.51      0.50      0.50       836
weighted avg       0.51      0.50      0.50       836

(D) Accuracy: 0.4964114832535885
(D) Macro-average F1: 0.4997214149548104
(D) Weighted-average F1: 0.49982742807416336
--------------------
Top DT Model:
(A) Model Description:
Top DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[127  29 113]
 [ 46 196  35]
 [125  45 120]]
Precision Scores: [0.4261745  0.72592593 0.44776119]
Recall Scores: [0.47211896 0.70758123 0.4137931 ]
F1 Scores: [0.44797178 0.7166362  0.43010753]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.43      0.47      0.45       269
           F       0.73      0.71      0.72       277
           I       0.45      0.41      0.43       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.53       836
weighted avg       0.53      0.53      0.53       836

(D) Accuracy: 0.5299043062200957
(D) Macro-average F1: 0.5315718352091401
(D) Weighted-average F1: 0.5307940414567187
--------------------
Base MLP Model:
(A) Model Description:
Base MLP Model with hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 66  30 173]
 [ 11 211  55]
 [ 71  50 169]]
Precision Scores: [0.44594595 0.72508591 0.4256927 ]
Recall Scores: [0.24535316 0.76173285 0.58275862]
F1 Scores: [0.31654676 0.74295775 0.49199418]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.45      0.25      0.32       269
           F       0.73      0.76      0.74       277
           I       0.43      0.58      0.49       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.52       836
weighted avg       0.53      0.53      0.52       836

(D) Accuracy: 0.5334928229665071
(D) Macro-average F1: 0.5171662288841662
(D) Weighted-average F1: 0.5186946009696302
--------------------
Top MLP Model:
(A) Model Description:
Top MLP Model with hyperparameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (30, 50), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 91  29 149]
 [ 17 218  42]
 [104  53 133]]
Precision Scores: [0.42924528 0.72666667 0.41049383]
Recall Scores: [0.33828996 0.78700361 0.45862069]
F1 Scores: [0.37837838 0.75563258 0.43322476]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.43      0.34      0.38       269
           F       0.73      0.79      0.76       277
           I       0.41      0.46      0.43       290

    accuracy                           0.53       836
   macro avg       0.52      0.53      0.52       836
weighted avg       0.52      0.53      0.52       836

(D) Accuracy: 0.5287081339712919
(D) Macro-average F1: 0.5224119054670203
(D) Weighted-average F1: 0.5224033352155157
--------------------
Base DT Model:
(A) Model Description:
Base DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[103  34 132]
 [ 41 175  61]
 [115  40 135]]
Precision Scores: [0.3976834  0.70281124 0.41158537]
Recall Scores: [0.38289963 0.63176895 0.46551724]
F1 Scores: [0.39015152 0.66539924 0.4368932 ]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.40      0.38      0.39       269
           F       0.70      0.63      0.67       277
           I       0.41      0.47      0.44       290

    accuracy                           0.49       836
   macro avg       0.50      0.49      0.50       836
weighted avg       0.50      0.49      0.50       836

(D) Accuracy: 0.49401913875598086
(D) Macro-average F1: 0.4974813195262455
(D) Weighted-average F1: 0.497566239300937
--------------------
Top DT Model:
(A) Model Description:
Top DT Model with hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
(B) Confusion Matrix:
[[129  29 111]
 [ 46 196  35]
 [126  45 119]]
Precision Scores: [0.42857143 0.72592593 0.4490566 ]
Recall Scores: [0.4795539  0.70758123 0.41034483]
F1 Scores: [0.45263158 0.7166362  0.42882883]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.43      0.48      0.45       269
           F       0.73      0.71      0.72       277
           I       0.45      0.41      0.43       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.53       836
weighted avg       0.53      0.53      0.53       836

(D) Accuracy: 0.5311004784688995
(D) Macro-average F1: 0.5326988684055941
(D) Weighted-average F1: 0.5318498585983786
--------------------
Base MLP Model:
(A) Model Description:
Base MLP Model with hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 66  30 173]
 [ 11 211  55]
 [ 71  50 169]]
Precision Scores: [0.44594595 0.72508591 0.4256927 ]
Recall Scores: [0.24535316 0.76173285 0.58275862]
F1 Scores: [0.31654676 0.74295775 0.49199418]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.45      0.25      0.32       269
           F       0.73      0.76      0.74       277
           I       0.43      0.58      0.49       290

    accuracy                           0.53       836
   macro avg       0.53      0.53      0.52       836
weighted avg       0.53      0.53      0.52       836

(D) Accuracy: 0.5334928229665071
(D) Macro-average F1: 0.5171662288841662
(D) Weighted-average F1: 0.5186946009696302
--------------------
Top MLP Model:
(A) Model Description:
Top MLP Model with hyperparameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (30, 50), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}
(B) Confusion Matrix:
[[ 10  19 240]
 [  3 181  93]
 [ 21  33 236]]
Precision Scores: [0.29411765 0.77682403 0.41476274]
Recall Scores: [0.03717472 0.6534296  0.8137931 ]
F1 Scores: [0.0660066  0.70980392 0.54947614]
(C) Precision, Recall, and F1:
              precision    recall  f1-score   support

           M       0.29      0.04      0.07       269
           F       0.78      0.65      0.71       277
           I       0.41      0.81      0.55       290

    accuracy                           0.51       836
   macro avg       0.50      0.50      0.44       836
weighted avg       0.50      0.51      0.45       836

(D) Accuracy: 0.5107655502392344
(D) Macro-average F1: 0.4417622190898129
(D) Weighted-average F1: 0.4470329437965115
--------------------
