{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "import numpy as np \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision_recall_f1 = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_avg_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_avg_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'y_pred': y_pred,  # Add this line\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'precision_recall_f1': precision_recall_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_avg_f1': macro_avg_f1,\n",
    "        'weighted_avg_f1': weighted_avg_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Load Penguin Dataset and Prepare Data\n",
    "penguin_path = r'C:\\Users\\ibrah\\OneDrive\\Desktop\\COMP472-A1-datasets\\penguins.csv'\n",
    "penguin_data = pd.read_csv(penguin_path)\n",
    "\n",
    "# Convert features into 1-hot vectors\n",
    "categorical_columns = ['island', 'sex']\n",
    "penguin_categorical = penguin_data[categorical_columns]\n",
    "encoder = OneHotEncoder()\n",
    "one_hot_encoded = encoder.fit_transform(penguin_categorical).toarray()\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "penguin_data_encoded = pd.concat([penguin_data, one_hot_df], axis=1).drop(categorical_columns, axis=1)\n",
    "\n",
    "# Split the dataset\n",
    "penguin_features = penguin_data_encoded.drop('species', axis=1)\n",
    "penguin_target = penguin_data_encoded['species']\n",
    "penguin_X_train, penguin_X_test, penguin_y_train, penguin_y_test = train_test_split(\n",
    "    penguin_features, penguin_target, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Define models and their hyperparameter grids\n",
    "models = {\n",
    "    \"Base DT\": DecisionTreeClassifier(),\n",
    "    \"Top DT\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"Base MLP\": MLPClassifier(random_state=42, max_iter=1000),\n",
    "    \"Top MLP\": MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', hidden_layer_sizes=(30, 50), solver='adam', max_iter=1000),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Store the test data outside the loop\n",
    "class_labels = ['Adelie', 'Chinstrap', 'Gentoo']\n",
    "test_data = (penguin_y_test, class_labels)\n",
    "\n",
    "# Repeat the process 5 times for each model\n",
    "performance_results = []\n",
    "\n",
    "for _ in range(5):\n",
    "    for model_name, model in models.items():\n",
    "        # If it's a grid search model, perform the grid search\n",
    "        if isinstance(model, GridSearchCV):\n",
    "            model.fit(penguin_X_train, penguin_y_train)\n",
    "            best_params = model.best_params_\n",
    "            best_model = model.best_estimator_\n",
    "        else:\n",
    "            # For non-grid search models, use the model as is\n",
    "            best_params = model.get_params()\n",
    "            best_model = model\n",
    "\n",
    "        # Evaluate the model\n",
    "        model_results = evaluate_model(best_model, penguin_X_train, penguin_X_test, penguin_y_train, penguin_y_test)\n",
    "\n",
    "        # Append the results to the performance list\n",
    "        performance_results.append({\n",
    "            'model_name': model_name,\n",
    "            'best_model': best_model,\n",
    "            'results': model_results\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy: 0.8067164179104477\n",
      "Variance accuracy: 0.04411840053464023\n",
      "\n",
      "Avg macro_avg_f1: 0.7269561455426129\n",
      "Variance macro_avg_f1: 0.0932304917184717\n",
      "\n",
      "Avg weighted_avg_f1: 0.7627928599327615\n",
      "Variance weighted_avg_f1: 0.07348463092003907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6\n",
    "# Calculate and print the average and variance for each metric\n",
    "for metric in ['accuracy', 'macro_avg_f1', 'weighted_avg_f1']:\n",
    "    metric_values = [result['results'][metric] for result in performance_results if metric in result['results']]\n",
    "    average_metric = np.mean(metric_values)\n",
    "    variance_metric = np.var(metric_values)\n",
    "\n",
    "    print(f\"Avg {metric}: {average_metric}\")\n",
    "    print(f\"Variance {metric}: {variance_metric}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 (Updated)\n",
    "# Save the results to a file\n",
    "with open(\"penguin_performance.txt\", \"a\") as file:\n",
    "    for result in performance_results:\n",
    "        model_name = result['model_name']\n",
    "        best_model = result['best_model']  # Use the stored best model\n",
    "        model_results = result['results']\n",
    "\n",
    "        file.write(f\"{model_name} Model:\\n\")\n",
    "        file.write(\"(A) Model Description:\\n{} Model with hyperparameters: {}\\n\".format(model_name, best_model.get_params()))\n",
    "        file.write(\"(B) Confusion Matrix:\\n{}\\n\".format(model_results['conf_matrix']))\n",
    "\n",
    "        # Additional formatting for precision, recall, and f1\n",
    "        precision, recall, f1, _ = model_results['precision_recall_f1']\n",
    "        file.write(\"Precision Scores: {}\\n\".format(precision))\n",
    "        file.write(\"Recall Scores: {}\\n\".format(recall))\n",
    "        file.write(\"F1 Scores: {}\\n\".format(f1))\n",
    "\n",
    "        # Calculate the classification report\n",
    "        report_str = classification_report(\n",
    "            test_data[0], y_pred=model_results['y_pred'], target_names=class_labels, zero_division=1\n",
    "        )\n",
    "        file.write(\"(C) Precision, Recall, and F1:\\n{}\\n\".format(report_str))\n",
    "\n",
    "        file.write(\"(D) Accuracy: {}\\n\".format(model_results['accuracy']))\n",
    "        file.write(\"(D) Macro-average F1: {}\\n\".format(model_results['macro_avg_f1']))\n",
    "        file.write(\"(D) Weighted-average F1: {}\\n\".format(model_results['weighted_avg_f1']))\n",
    "\n",
    "        file.write(\"--------------------\\n\")  # Separator\n",
    "\n",
    "# Ensure you close the file to save the changes\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "abalone_path = r'C:\\Users\\ibrah\\OneDrive\\Desktop\\COMP472-A1-datasets\\abalone.csv'\n",
    "abalone_data = pd.read_csv(abalone_path)\n",
    "\n",
    "# Split the dataset into features and target\n",
    "abalone_features = abalone_data.drop('Type', axis=1)\n",
    "abalone_target = abalone_data['Type']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "abalone_X_train, abalone_X_test, abalone_y_train, abalone_y_test = train_test_split(\n",
    "    abalone_features, abalone_target, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    \"Base DT\": DecisionTreeClassifier(),\n",
    "    \"Top DT\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"Base MLP\": MLPClassifier(random_state=42, max_iter=1000),\n",
    "    \"Top MLP\": MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', hidden_layer_sizes=(30, 50), solver='adam', max_iter=1000),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the test data outside the loop\n",
    "test_data = (abalone_y_test, ['M', 'F', 'I'])  # The target variable is 'Type'\n",
    "\n",
    "# Repeat the process 5 times for each model\n",
    "performance_results = []\n",
    "\n",
    "for _ in range(5):\n",
    "    for model_name, model in models.items():\n",
    "        # If it's a grid search model, perform the grid search\n",
    "        if isinstance(model, GridSearchCV):\n",
    "            model.fit(abalone_X_train, abalone_y_train)\n",
    "            best_params = model.best_params_\n",
    "            best_model = model.best_estimator_\n",
    "        else:\n",
    "            # For non-grid search models, use the model as is\n",
    "            best_params = model.get_params()\n",
    "            best_model = model\n",
    "\n",
    "        # Evaluate the model\n",
    "        model_results = evaluate_model(best_model, abalone_X_train, abalone_X_test, abalone_y_train, abalone_y_test)\n",
    "\n",
    "        # Append the results to the performance list\n",
    "        performance_results.append({\n",
    "            'model_name': model_name,\n",
    "            'best_model': best_model,\n",
    "            'results': model_results\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy: 0.5206937799043062\n",
      "Variance accuracy: 0.00030893008401822273\n",
      "\n",
      "Avg macro_avg_f1: 0.5102026952042623\n",
      "Variance macro_avg_f1: 0.0004946265815827906\n",
      "\n",
      "Avg weighted_avg_f1: 0.5108571533506223\n",
      "Variance weighted_avg_f1: 0.0004424166763034035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average and variance for each metric\n",
    "for metric in ['accuracy', 'macro_avg_f1', 'weighted_avg_f1']:\n",
    "    metric_values = [result['results'][metric] for result in performance_results if metric in result['results']]\n",
    "    average_metric = np.mean(metric_values)\n",
    "    variance_metric = np.var(metric_values)\n",
    "\n",
    "    print(f\"Avg {metric}: {average_metric}\")\n",
    "    print(f\"Variance {metric}: {variance_metric}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a file\n",
    "with open(\"abalone_performance.txt\", \"a\") as file:\n",
    "    for result in performance_results:\n",
    "        model_name = result['model_name']\n",
    "        best_model = result['best_model']  # Use the stored best model\n",
    "        model_results = result['results']\n",
    "\n",
    "        file.write(f\"{model_name} Model:\\n\")\n",
    "        file.write(\"(A) Model Description:\\n{} Model with hyperparameters: {}\\n\".format(model_name, best_model.get_params()))\n",
    "        file.write(\"(B) Confusion Matrix:\\n{}\\n\".format(model_results['conf_matrix']))\n",
    "\n",
    "        # Additional formatting for precision, recall, and f1\n",
    "        precision, recall, f1, _ = model_results['precision_recall_f1']\n",
    "        file.write(\"Precision Scores: {}\\n\".format(precision))\n",
    "        file.write(\"Recall Scores: {}\\n\".format(recall))\n",
    "        file.write(\"F1 Scores: {}\\n\".format(f1))\n",
    "\n",
    "        # Calculate the classification report\n",
    "        report_str = classification_report(\n",
    "            test_data[0], y_pred=model_results['y_pred'], target_names=test_data[1], zero_division=1\n",
    "        )\n",
    "        file.write(\"(C) Precision, Recall, and F1:\\n{}\\n\".format(report_str))\n",
    "\n",
    "        file.write(\"(D) Accuracy: {}\\n\".format(model_results['accuracy']))\n",
    "        file.write(\"(D) Macro-average F1: {}\\n\".format(model_results['macro_avg_f1']))\n",
    "        file.write(\"(D) Weighted-average F1: {}\\n\".format(model_results['weighted_avg_f1']))\n",
    "\n",
    "        file.write(\"--------------------\\n\")  # Separator\n",
    "\n",
    "# Ensure you close the file to save the changes\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
