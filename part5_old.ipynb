{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each of the 4 classifiers above 4(a), 4(b), 4(c) and 4(d), append the following information in a file called\n",
    "penguin-performance.txt / abalone-performance.txt: (to make it easier for the TAs, make sure that\n",
    "your output for each sub-question below is clearly marked in your output file, using the headings (A),\n",
    "(B) . . .)\n",
    "(A) a clear separator (a sequence of hyphens or stars) and a string clearly describing the model (e.g. the\n",
    "model name + hyper-parameter values that you changed). In the case of Top-DT and Top-MLP,\n",
    "display the best hyperparameters found by the gridsearch.\n",
    "(B) the confusion matrix\n",
    "(C) the precision, recall, and F1-measure for each class\n",
    "(D) the accuracy, macro-average F1 and weighted-average F1 of the model\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from io import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Penguin dataset\n",
    "penguin_path = r'C:\\Users\\ibrah\\OneDrive\\Desktop\\COMP472-A1-datasets\\penguins.csv'\n",
    "penguin_data = pd.read_csv(penguin_path)\n",
    "\n",
    "# For Penguin dataset\n",
    "penguin_features = pd.get_dummies(penguin_data.drop('species', axis=1))  # One-hot encode categorical features\n",
    "penguin_target = penguin_data['species']\n",
    "\n",
    "penguin_X_train, penguin_X_test, penguin_y_train, penguin_y_test = train_test_split(\n",
    "    penguin_features, penguin_target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Base-DT\n",
    "base_dt = DecisionTreeClassifier(random_state=42)\n",
    "base_dt.fit(penguin_X_train, penguin_y_train)\n",
    "base_dt_predictions = base_dt.predict(penguin_X_test)\n",
    "\n",
    "# Output file for Base-DT\n",
    "output_file_base_dt = 'penguin-performance.txt'\n",
    "\n",
    "# Append information to the output file\n",
    "with open(output_file_base_dt, 'a') as f:\n",
    "    f.write(\"Base-DT Model:\\n\")\n",
    "    f.write(\"Hyperparameters: Default\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(confusion_matrix(penguin_y_test, base_dt_predictions)) + \"\\n\")\n",
    "    f.write(\"Precision, Recall, and F1:\\n\")\n",
    "    f.write(str(classification_report(penguin_y_test, base_dt_predictions)) + \"\\n\")\n",
    "    f.write(\"Accuracy, Macro-average F1, Weighted-average F1:\\n\")\n",
    "    f.write(str(accuracy_score(penguin_y_test, base_dt_predictions)) + \"\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Top-DT\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10],  # Update with your choices\n",
    "    'min_samples_split': [2, 5, 10]  # Update with your choices\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=5, scoring='accuracy')\n",
    "grid_search_dt.fit(penguin_X_train, penguin_y_train)\n",
    "top_dt = grid_search_dt.best_estimator_\n",
    "top_dt_predictions = top_dt.predict(penguin_X_test)\n",
    "\n",
    "# Output file for Top-DT\n",
    "output_file_top_dt = 'penguin-performance.txt'\n",
    "\n",
    "# Append information to the output file\n",
    "with open(output_file_top_dt, 'a') as f:\n",
    "    f.write(\"Top-DT Model:\\n\")\n",
    "    f.write(\"Best Hyperparameters: {}\\n\".format(grid_search_dt.best_params_))\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(confusion_matrix(penguin_y_test, top_dt_predictions)) + \"\\n\")\n",
    "    f.write(\"Precision, Recall, and F1:\\n\")\n",
    "    f.write(str(classification_report(penguin_y_test, top_dt_predictions)) + \"\\n\")\n",
    "    f.write(\"Accuracy, Macro-average F1, Weighted-average F1:\\n\")\n",
    "    f.write(str(accuracy_score(penguin_y_test, top_dt_predictions)) + \"\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibrah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ibrah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ibrah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# (c) Base-MLP\n",
    "base_mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', max_iter=500, random_state=42)\n",
    "base_mlp.fit(penguin_X_train, penguin_y_train)\n",
    "base_mlp_predictions = base_mlp.predict(penguin_X_test)\n",
    "\n",
    "# Output file for Base-MLP\n",
    "output_file_base_mlp = 'penguin-performance.txt'\n",
    "\n",
    "# Append information to the output file\n",
    "with open(output_file_base_mlp, 'a') as f:\n",
    "    f.write(\"Base-MLP Model:\\n\")\n",
    "    f.write(\"Hyperparameters: Default\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(confusion_matrix(penguin_y_test, base_mlp_predictions)) + \"\\n\")\n",
    "    f.write(\"Precision, Recall, and F1:\\n\")\n",
    "    f.write(str(classification_report(penguin_y_test, base_mlp_predictions)) + \"\\n\")\n",
    "    f.write(\"Accuracy, Macro-average F1, Weighted-average F1:\\n\")\n",
    "    f.write(str(accuracy_score(penguin_y_test, base_mlp_predictions)) + \"\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibrah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ibrah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ibrah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\ibrah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# (d) Top-MLP\n",
    "param_grid_mlp = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'max_iter': [500, 1000]\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(MLPClassifier(random_state=42), param_grid_mlp, cv=5, scoring='accuracy')\n",
    "grid_search_mlp.fit(penguin_X_train, penguin_y_train)\n",
    "top_mlp = grid_search_mlp.best_estimator_\n",
    "top_mlp_predictions = top_mlp.predict(penguin_X_test)\n",
    "\n",
    "# Output file for Top-MLP\n",
    "output_file_top_mlp = 'penguin-performance.txt'\n",
    "\n",
    "# Append information to the output file\n",
    "with open(output_file_top_mlp, 'a') as f:\n",
    "    f.write(\"Top-MLP Model:\\n\")\n",
    "    f.write(\"Best Hyperparameters: {}\\n\".format(grid_search_mlp.best_params_))\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(str(confusion_matrix(penguin_y_test, top_mlp_predictions)) + \"\\n\")\n",
    "    f.write(\"Precision, Recall, and F1:\\n\")\n",
    "    f.write(str(classification_report(penguin_y_test, top_mlp_predictions)) + \"\\n\")\n",
    "    f.write(\"Accuracy, Macro-average F1, Weighted-average F1:\\n\")\n",
    "    f.write(str(accuracy_score(penguin_y_test, top_mlp_predictions)) + \"\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
